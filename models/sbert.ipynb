{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re \n",
    "import torch\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(sentence):\n",
    "#     sentence=str(sentence)\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence=sentence.replace('{html}',\"\")\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', sentence)\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)  \n",
    "#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "#     stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "# print(\"--------PROCESSING TRAINING DATA--------\")\n",
    "# processed_df = df.copy()\n",
    "# processed_df['resume_text'] = processed_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['job_description_text'] = processed_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['combined_text'] = processed_df['resume_text'] + ' ' + processed_df['job_description_text']\n",
    "# processed_df['label'] = le.fit_transform(processed_df['label'])\n",
    "\n",
    "# print(\"--------PROCESSING TEST DATA--------\")\n",
    "# test_df = pd.read_csv('data/test.csv')\n",
    "# processed_test_df = test_df.copy()\n",
    "\n",
    "# processed_test_df['resume_text'] = processed_test_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['job_description_text'] = processed_test_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['combined_text'] = processed_test_df['resume_text'] + ' ' + processed_test_df['job_description_text']\n",
    "# processed_test_df['label'] = le.transform(processed_test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.to_csv('data/processed_train.csv', index=False)\n",
    "# processed_test_df.to_csv('data/processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making an evaluation set from the training set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_processed = pd.read_csv('data/processed_train.csv')\n",
    "# train_data, eval_data = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "# train_data.to_csv('data/processed_train.csv', index=False)\n",
    "# eval_data.to_csv('data/processed_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv('data/processed_train.csv')\n",
    "processed_eval = pd.read_csv('data/processed_eval.csv')\n",
    "processed_test_df = pd.read_csv('data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pretrained model (BERT in this case) and its tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with columns 'resume_text', 'job_description_text', and 'label'\n",
    "        \"\"\"\n",
    "        self.sentences_a = df['resume_text'].tolist()\n",
    "        self.sentences_b = df['job_description_text'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding_a = self.tokenizer(\n",
    "            self.sentences_a[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding_b = self.tokenizer(\n",
    "            self.sentences_b[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids_a': encoding_a['input_ids'].squeeze(0),\n",
    "            'attention_mask_a': encoding_a['attention_mask'].squeeze(0),\n",
    "            'input_ids_b': encoding_b['input_ids'].squeeze(0),\n",
    "            'attention_mask_b': encoding_b['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>job_description_text</th>\n",
       "      <th>label</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarya business management graduate signific...</td>\n",
       "      <td>position title senior accountant organization ...</td>\n",
       "      <td>2</td>\n",
       "      <td>summarya business management graduate signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>professional profilecapable international tax ...</td>\n",
       "      <td>rolegaming business analystresponsibilities ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>professional profilecapable international tax ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>professional profilehighly motivated sales ass...</td>\n",
       "      <td>handle accounting responsibilities growing pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>professional profilehighly motivated sales ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summaryorganized motivated employee eager appl...</td>\n",
       "      <td>client growing medical device company located ...</td>\n",
       "      <td>1</td>\n",
       "      <td>summaryorganized motivated employee eager appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summaryemployed navy civilian electrical engin...</td>\n",
       "      <td>seeking detail oriented analytical individual ...</td>\n",
       "      <td>1</td>\n",
       "      <td>summaryemployed navy civilian electrical engin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  summarya business management graduate signific...   \n",
       "1  professional profilecapable international tax ...   \n",
       "2  professional profilehighly motivated sales ass...   \n",
       "3  summaryorganized motivated employee eager appl...   \n",
       "4  summaryemployed navy civilian electrical engin...   \n",
       "\n",
       "                                job_description_text  label  \\\n",
       "0  position title senior accountant organization ...      2   \n",
       "1  rolegaming business analystresponsibilities ab...      1   \n",
       "2  handle accounting responsibilities growing pro...      1   \n",
       "3  client growing medical device company located ...      1   \n",
       "4  seeking detail oriented analytical individual ...      1   \n",
       "\n",
       "                                       combined_text  \n",
       "0  summarya business management graduate signific...  \n",
       "1  professional profilecapable international tax ...  \n",
       "2  professional profilehighly motivated sales ass...  \n",
       "3  summaryorganized motivated employee eager appl...  \n",
       "4  summaryemployed navy civilian electrical engin...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- DATA LOADERS -----------------\n",
    "\n",
    "train_dataset = SentencePairDataset(processed_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "eval_dataset = SentencePairDataset(processed_eval, tokenizer)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "test_dataset = SentencePairDataset(processed_test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size * 3, 3)  # 3 classes for classification\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "        outputs_a = self.bert(input_ids_a, attention_mask=attention_mask_a)\n",
    "        pooled_output_a = outputs_a.last_hidden_state[:, 0, :]  # CLS token for sentence A\n",
    "\n",
    "        outputs_b = self.bert(input_ids_b, attention_mask=attention_mask_b)\n",
    "        pooled_output_b = outputs_b.last_hidden_state[:, 0, :]  # CLS token for sentence B\n",
    "\n",
    "        abs_diff = torch.abs(pooled_output_a - pooled_output_b)\n",
    "        combined = torch.cat((pooled_output_a, pooled_output_b, abs_diff), dim=1)\n",
    "        logits = self.fc(combined)\n",
    "\n",
    "        return logits  # Return raw logits; softmax will be applied in loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available devices and assign the appropriate one\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SBERT model and send it to the selected device\n",
    "sbert = SBERT(model).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sbert.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1154.1459\n",
      "Epoch 1, Eval Loss: 223.0265, Eval Accuracy: 0.6573\n",
      "Epoch 2, Train Loss: 840.1487\n",
      "Epoch 2, Eval Loss: 195.2219, Eval Accuracy: 0.7182\n",
      "Epoch 3, Train Loss: 722.9590\n",
      "Epoch 3, Eval Loss: 182.5196, Eval Accuracy: 0.7382\n"
     ]
    }
   ],
   "source": [
    "# Training loop with eval set included\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    sbert.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on eval set\n",
    "    sbert.eval()\n",
    "    eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids_a = batch['input_ids_a'].to(device)\n",
    "            attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "            input_ids_b = batch['input_ids_b'].to(device)\n",
    "            attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    eval_accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Epoch {epoch + 1}, Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 396.4787, Test Accuracy: 0.5571\n"
     ]
    }
   ],
   "source": [
    "# Final test performance check\n",
    "sbert.eval()\n",
    "test_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Eval accuracy =  0.7045666666666666\n",
      "Final Test accuracy =  0.5571\n"
     ]
    }
   ],
   "source": [
    "eval_accuracy = [0.6573,0.7182,0.7382]\n",
    "test_accuracy = 0.5571\n",
    "\n",
    "print(\"Final Eval accuracy = \", sum(eval_accuracy) / len(eval_accuracy))\n",
    "print(\"Final Test accuracy = \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below can be used to checkpoint/save the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "# }, \"model_and_optimizer_nn_1.pth\")\n",
    "\n",
    "\n",
    "# Load both model and optimizer state_dicts\n",
    "# checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobfit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
