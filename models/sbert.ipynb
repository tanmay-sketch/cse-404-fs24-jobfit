{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/jobfit_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re \n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(sentence):\n",
    "#     sentence=str(sentence)\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence=sentence.replace('{html}',\"\")\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', sentence)\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)  \n",
    "#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "#     stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "# print(\"--------PROCESSING TRAINING DATA--------\")\n",
    "# processed_df = df.copy()\n",
    "# processed_df['resume_text'] = processed_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['job_description_text'] = processed_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['combined_text'] = processed_df['resume_text'] + ' ' + processed_df['job_description_text']\n",
    "# processed_df['label'] = le.fit_transform(processed_df['label'])\n",
    "\n",
    "# print(\"--------PROCESSING TEST DATA--------\")\n",
    "# test_df = pd.read_csv('data/test.csv')\n",
    "# processed_test_df = test_df.copy()\n",
    "\n",
    "# processed_test_df['resume_text'] = processed_test_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['job_description_text'] = processed_test_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['combined_text'] = processed_test_df['resume_text'] + ' ' + processed_test_df['job_description_text']\n",
    "# processed_test_df['label'] = le.transform(processed_test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.to_csv('data/processed_train.csv', index=False)\n",
    "# processed_test_df.to_csv('data/processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making an evaluation set from the training set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_processed = pd.read_csv('data/processed_train.csv')\n",
    "# train_data, eval_data = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "# train_data.to_csv('data/processed_train.csv', index=False)\n",
    "# eval_data.to_csv('data/processed_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv('../data/processed_train.csv')\n",
    "processed_eval = pd.read_csv('../data/processed_eval.csv')\n",
    "processed_test_df = pd.read_csv('../data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pretrained model (BERT in this case) and its tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with columns 'resume_text', 'job_description_text', and 'label'\n",
    "        \"\"\"\n",
    "        self.sentences_a = df['resume_text'].tolist()\n",
    "        self.sentences_b = df['job_description_text'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        string += 'Dataset with {} samples\\n'.format(len(self.labels))\n",
    "        string += 'Sample:\\n'\n",
    "        string += 'Sample 1 Resume: {}\\n'.format(self.sentences_a[0])\n",
    "        string += 'Sample 1 Job Description: {}\\n'.format(self.sentences_b[0])\n",
    "        string += 'Length of Sample 1: {}\\n'.format(len(self.sentences_b[0].split()))\n",
    "        string += 'Label: {}\\n'.format(self.labels[0])\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding_a = self.tokenizer(\n",
    "            self.sentences_a[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding_b = self.tokenizer(\n",
    "            self.sentences_b[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids_a': encoding_a['input_ids'].squeeze(0),\n",
    "            'attention_mask_a': encoding_a['attention_mask'].squeeze(0),\n",
    "            'input_ids_b': encoding_b['input_ids'].squeeze(0),\n",
    "            'attention_mask_b': encoding_b['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1483"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_df.iloc[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 4992 samples\n",
      "Sample:\n",
      "Sample 1 Resume: summarya business management graduate significant experience disability services human resources finance department seeking apply abilities position human resource department proven ability deal multiple tasks efficiently maintain organization highlightsextensive customer service skillsable retain confidentiality paperwork informationflexible team player quick learner interested new technologiesable meet deadlines handle stressful situations professional mannermicrosoft office word excel access outlookknowledge state accounting system mmars system people soft system client tracking systemexcellent communicator written verbaloffice experience ability create reports analyze data manipulate data experienceaccountant topresentfreeport mcmoran copper gold inc tucson provide administrative support finance unit maintain orderly filing system various departmental units oversee maintain proper appropriate systems storing financial records documents utilized finance unit assist contracts accounts payable department clerical duties faxing filing copying scanning assisting creation editing documents spreadsheets powerpoint presentations process travel reimbursements incoming payment vouchers financial documents timely efficient manner assist payroll projections made pay periods end fiscal year analyze financial reports trends major programs determine effects spending prepare financial projections accordingly monitor internal budget compliance mmars system create quarterly report agency chief financial officer time usage prepare purchase orders payment vouchers processing compile reports senior financial analyst accurate manner maintain organized categorized excel spreadsheet auditing purposes provide support contracts department entering data internal database mail merging contract information create mmars contracts signoff sheets update contract new fiscal year information confer agency personnel outside agencies via mail telephone resolve finance discrepancies timely manner intern dlz logan assisted human resources department assisted liaison collecting various forms eprs gic retirement among others couriering important documents various state agencies helping create fill necessary forms maintained strictly confidential information file documents performed clerical duties various units within agency entered weekly time sheet information staff various units supervised high school interns various duties projects obtained proper signatory authorization managers process documents assisted organizing running mcb summer internship opening closing ceremonies making name tags certificates attending planning meetings working sign tables providing sighted guide consumers event attendees visually impaired organized prepared marketing materials various conferences meetings events translated information spanish staff clients related various projects coordinate prepare marketing materials various conferences meetings events provided safe transportation services mcb staff various consumer appointments security officer danbury health systems poughkeepsie prepared written reports complaints incidents occurred event provided customer service ensured safety fans garden property provided assisted customers location seating area educationbachelor science business management accounting psychology expected inmay tobridgewater state university bridgewater magpa business managementminor psychology accounting finance skillsaccounting system accounts payable administrative support agency auditing budget clerical closing conferences contracts copying client clients customer service customer service skills database editing mail english faxing filing finance financial financial analyst financial reports forms human resources marketing materials meetings access excel mail microsoft office outlook powerpoint presentations word communicator organizing payroll people soft personnel quick learner safety scanning spanish spreadsheets spreadsheet tables team player telephone transportation written\n",
      "Sample 1 Job Description: position title senior accountant organization jewish family service san diego department family community services division organization jewish family service san diego position type full time hoursweek exempt salary year total compensation addition standard pay compensation position includes comprehensive low cost healthcare coverage employees generous employer contributions employer covered life insurance time away work able take time away work critical bringing best self work time benefits position include generous paid vacation time sick leave paid holidays including federal jewish floating holidays wellness days taken time year support employees mental wellness position overview senior accountant integral part financial planning function within finance accounting team position works closely range departments programs including program directors senior staff candidate must self starter diligent proactive detail oriented expected contribute significantly finance accounting team direction senior directorcontroller position support agency fiscal operations transactional processing financial analysis annual budget development monitoring cost allocation monthly forecasts program annual audit support implementation financial compliance controls senior accountant serves key resource organization provides daily financial oversight ensure financial records complete accurate comply generally accepted accounting principles policies regulations facilitate solid decision support responsibilities deliver monthly financial reporting analytics insight performance metrics benchmarking progress targets initiatives strategies conduct analytical reviews departmental programmatic activities support complex financial budgetary forecasting evaluates alternatives makes recommendations develop understanding organization wide departments programs funding streams andor cost structure assist analysis used decision making recommendations management programs analyze financial operational results understand report underlying causes performance variance assist agency budgeting forecasts scenario planning based changing assumptions revenue personnel direct project costs perform budget actual variance analysis identify causes adjustments needed trends personnel reclassifications potential budget shortfalls cost overruns opportunities cost savings assist weekly cashflow management reports projections prepare monthly quarterly annual hoc financial statements actual projected financial positions ensure integrity key processes understanding systems flow transactions internal controls recommending efficiency effectiveness improvements work finance accounting team identify areas opportunities improvement support organizational management practices applied analysis evaluation development implementation programs policies procedures perform variance analyses account reconciliations needed assist month end closing year end closing reconciliations preparation external audit prepare necessary audit schedules needed hoc requests one projects duties assigned skillsexperienceabilities must bachelor degree accounting finance required minimum years accounting experience progressive relevant analytical budgeting analysis reporting experience minimum years nonprofit industry knowledge understanding cost methodologies budgeting concepts practices procedures accounting competencies including understanding revenue recognition understanding gaap compliance uniform guidance omb circulars excellent written verbal communication interpersonal skills strong analytical organizational time management skills work well team approach environment demonstrably strong judgement decision making skills ability multi task prioritize workflow advanced excel proficient word outlook skillsabilities wed like desire work fun friendly collaborative professional environment passion working non profit human services organization dedicated helping individuals need experience financial edge required preferred important notice incomplete submissions considered please phone calls please principals please local candidates relocation provided jewish family service san diegojewish family service san diego client centered impact driven organization working build stronger resilient community years jewish family service trusted resource entire community offering array services always life changing often life saving jewish family service believe employees backbone agency strive ensure employee treated dignity respect goal success come work jfs partner moving forward together learn jfs please visit jfssd org jewish family service equal opportunity employer applicants considered employment without attention race color religion sex sexual orientation gender identity national origin veteran disability status\n",
      "Length of Sample 1: 557\n",
      "Label: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(SentencePairDataset(processed_df, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- DATA LOADERS -----------------\n",
    "\n",
    "train_dataset = SentencePairDataset(processed_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "eval_dataset = SentencePairDataset(processed_eval, tokenizer)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "test_dataset = SentencePairDataset(processed_test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size * 3, 3)  # 3 classes for classification\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "        outputs_a = self.bert(input_ids_a, attention_mask=attention_mask_a)\n",
    "        pooled_output_a = outputs_a.last_hidden_state[:, 0, :]  # CLS token for sentence A\n",
    "\n",
    "        outputs_b = self.bert(input_ids_b, attention_mask=attention_mask_b)\n",
    "        pooled_output_b = outputs_b.last_hidden_state[:, 0, :]  # CLS token for sentence B\n",
    "\n",
    "        abs_diff = torch.abs(pooled_output_a - pooled_output_b)\n",
    "        combined = torch.cat((pooled_output_a, pooled_output_b, abs_diff), dim=1)\n",
    "        logits = self.fc(combined)\n",
    "\n",
    "        return logits  # Return raw logits; softmax will be applied in loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available devices and assign the appropriate one\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SBERT model and send it to the selected device\n",
    "sbert = SBERT(model).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sbert.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrandhi1\u001b[0m (\u001b[33mmakeai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/cse-404-fs24-jobfit/models/wandb/run-20241123_205210-a51wfql7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/makeai/jobfit/runs/a51wfql7' target=\"_blank\">sbert-training-1</a></strong> to <a href='https://wandb.ai/makeai/jobfit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/makeai/jobfit' target=\"_blank\">https://wandb.ai/makeai/jobfit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/makeai/jobfit/runs/a51wfql7' target=\"_blank\">https://wandb.ai/makeai/jobfit/runs/a51wfql7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define and initialize wandb\n",
    "config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 20,\n",
    "    \"model\": \"distilbert-base-uncased\",\n",
    "    \"dataset\": \"processed_train\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"jobfit\",\n",
    "    config=config,\n",
    "    name=\"sbert-training-1\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Access the config\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1129.0325\n",
      "Epoch 1, Eval Loss: 223.4205, Eval Accuracy: 0.6541\n",
      "Epoch 2, Train Loss: 828.9441\n",
      "Epoch 2, Eval Loss: 190.7120, Eval Accuracy: 0.7318\n",
      "Epoch 3, Train Loss: 709.2908\n",
      "Epoch 3, Eval Loss: 169.2036, Eval Accuracy: 0.7518\n",
      "Epoch 4, Train Loss: 605.6866\n",
      "Epoch 4, Eval Loss: 160.8488, Eval Accuracy: 0.7870\n",
      "Epoch 5, Train Loss: 512.0554\n",
      "Epoch 5, Eval Loss: 152.7054, Eval Accuracy: 0.7974\n",
      "Epoch 6, Train Loss: 432.3302\n",
      "Epoch 6, Eval Loss: 139.5454, Eval Accuracy: 0.8247\n",
      "Epoch 7, Train Loss: 368.8202\n",
      "Epoch 7, Eval Loss: 127.9034, Eval Accuracy: 0.8519\n",
      "Epoch 8, Train Loss: 324.3971\n",
      "Epoch 8, Eval Loss: 130.0263, Eval Accuracy: 0.8495\n",
      "Epoch 9, Train Loss: 282.1511\n",
      "Epoch 9, Eval Loss: 124.1237, Eval Accuracy: 0.8575\n",
      "Epoch 10, Train Loss: 246.2789\n",
      "Epoch 10, Eval Loss: 126.7270, Eval Accuracy: 0.8591\n",
      "Epoch 11, Train Loss: 213.1448\n",
      "Epoch 11, Eval Loss: 119.4215, Eval Accuracy: 0.8719\n",
      "Epoch 12, Train Loss: 202.7066\n",
      "Epoch 12, Eval Loss: 132.6694, Eval Accuracy: 0.8711\n",
      "Epoch 13, Train Loss: 187.2588\n",
      "Epoch 13, Eval Loss: 133.5500, Eval Accuracy: 0.8591\n",
      "Epoch 14, Train Loss: 165.5736\n",
      "Epoch 14, Eval Loss: 123.6026, Eval Accuracy: 0.8863\n",
      "Epoch 15, Train Loss: 158.8247\n",
      "Epoch 15, Eval Loss: 133.7383, Eval Accuracy: 0.8751\n",
      "Epoch 16, Train Loss: 145.2925\n",
      "Epoch 16, Eval Loss: 147.1909, Eval Accuracy: 0.8807\n",
      "Epoch 17, Train Loss: 129.3715\n",
      "Epoch 17, Eval Loss: 135.5854, Eval Accuracy: 0.8823\n",
      "Epoch 18, Train Loss: 119.1636\n",
      "Epoch 18, Eval Loss: 135.2897, Eval Accuracy: 0.8855\n",
      "Epoch 19, Train Loss: 118.7032\n",
      "Epoch 19, Eval Loss: 150.8983, Eval Accuracy: 0.8719\n",
      "Epoch 20, Train Loss: 106.2731\n",
      "Epoch 20, Eval Loss: 133.9353, Eval Accuracy: 0.8823\n"
     ]
    }
   ],
   "source": [
    "# Training loop with eval set included\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    sbert.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": loss / len(train_loader)\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on eval set\n",
    "    sbert.eval()\n",
    "    eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids_a = batch['input_ids_a'].to(device)\n",
    "            attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "            input_ids_b = batch['input_ids_b'].to(device)\n",
    "            attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    eval_accuracy = correct_predictions / total_predictions\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"eval_accuracy\": eval_accuracy\n",
    "    })\n",
    "    print(f\"Epoch {epoch + 1}, Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 752.4419, Test Accuracy: 0.5458\n"
     ]
    }
   ],
   "source": [
    "# Final test performance check\n",
    "sbert.eval()\n",
    "test_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "wandb.log({\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>eval_accuracy</td><td>▁▃▄▅▅▆▇▇▇▇██▇███████</td></tr><tr><td>eval_loss</td><td>█▆▄▄▃▂▂▂▁▁▁▂▂▁▂▃▂▂▃▂</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄█▇▅▆▃▄▂▃▁▃▃▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>eval_accuracy</td><td>0.88231</td></tr><tr><td>eval_loss</td><td>133.93529</td></tr><tr><td>test_accuracy</td><td>0.54576</td></tr><tr><td>test_loss</td><td>752.44194</td></tr><tr><td>train_loss</td><td>2e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sbert-training-1</strong> at: <a href='https://wandb.ai/makeai/jobfit/runs/a51wfql7' target=\"_blank\">https://wandb.ai/makeai/jobfit/runs/a51wfql7</a><br/> View project at: <a href='https://wandb.ai/makeai/jobfit' target=\"_blank\">https://wandb.ai/makeai/jobfit</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241123_205210-a51wfql7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below can be used to checkpoint/save the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "# }, \"model_and_optimizer_nn_1.pth\")\n",
    "\n",
    "\n",
    "# Load both model and optimizer state_dicts\n",
    "# checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobfit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
