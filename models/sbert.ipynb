{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re \n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(sentence):\n",
    "#     sentence=str(sentence)\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence=sentence.replace('{html}',\"\")\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', sentence)\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)  \n",
    "#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "#     stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "# print(\"--------PROCESSING TRAINING DATA--------\")\n",
    "# processed_df = df.copy()\n",
    "# processed_df['resume_text'] = processed_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['job_description_text'] = processed_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['combined_text'] = processed_df['resume_text'] + ' ' + processed_df['job_description_text']\n",
    "# processed_df['label'] = le.fit_transform(processed_df['label'])\n",
    "\n",
    "# print(\"--------PROCESSING TEST DATA--------\")\n",
    "# test_df = pd.read_csv('data/test.csv')\n",
    "# processed_test_df = test_df.copy()\n",
    "\n",
    "# processed_test_df['resume_text'] = processed_test_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['job_description_text'] = processed_test_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['combined_text'] = processed_test_df['resume_text'] + ' ' + processed_test_df['job_description_text']\n",
    "# processed_test_df['label'] = le.transform(processed_test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.to_csv('data/processed_train.csv', index=False)\n",
    "# processed_test_df.to_csv('data/processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making an evaluation set from the training set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_processed = pd.read_csv('data/processed_train.csv')\n",
    "# train_data, eval_data = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "# train_data.to_csv('data/processed_train.csv', index=False)\n",
    "# eval_data.to_csv('data/processed_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv('../data/processed_train.csv')\n",
    "processed_eval = pd.read_csv('../data/processed_eval.csv')\n",
    "processed_test_df = pd.read_csv('../data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a pretrained model (BERT in this case) and its tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with columns 'resume_text', 'job_description_text', and 'label'\n",
    "        \"\"\"\n",
    "        self.sentences_a = df['resume_text'].tolist()\n",
    "        self.sentences_b = df['job_description_text'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding_a = self.tokenizer(\n",
    "            self.sentences_a[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding_b = self.tokenizer(\n",
    "            self.sentences_b[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids_a': encoding_a['input_ids'].squeeze(0),\n",
    "            'attention_mask_a': encoding_a['attention_mask'].squeeze(0),\n",
    "            'input_ids_b': encoding_b['input_ids'].squeeze(0),\n",
    "            'attention_mask_b': encoding_b['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>job_description_text</th>\n",
       "      <th>label</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarya business management graduate signific...</td>\n",
       "      <td>position title senior accountant organization ...</td>\n",
       "      <td>2</td>\n",
       "      <td>summarya business management graduate signific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>professional profilecapable international tax ...</td>\n",
       "      <td>rolegaming business analystresponsibilities ab...</td>\n",
       "      <td>1</td>\n",
       "      <td>professional profilecapable international tax ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>professional profilehighly motivated sales ass...</td>\n",
       "      <td>handle accounting responsibilities growing pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>professional profilehighly motivated sales ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summaryorganized motivated employee eager appl...</td>\n",
       "      <td>client growing medical device company located ...</td>\n",
       "      <td>1</td>\n",
       "      <td>summaryorganized motivated employee eager appl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summaryemployed navy civilian electrical engin...</td>\n",
       "      <td>seeking detail oriented analytical individual ...</td>\n",
       "      <td>1</td>\n",
       "      <td>summaryemployed navy civilian electrical engin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  summarya business management graduate signific...   \n",
       "1  professional profilecapable international tax ...   \n",
       "2  professional profilehighly motivated sales ass...   \n",
       "3  summaryorganized motivated employee eager appl...   \n",
       "4  summaryemployed navy civilian electrical engin...   \n",
       "\n",
       "                                job_description_text  label  \\\n",
       "0  position title senior accountant organization ...      2   \n",
       "1  rolegaming business analystresponsibilities ab...      1   \n",
       "2  handle accounting responsibilities growing pro...      1   \n",
       "3  client growing medical device company located ...      1   \n",
       "4  seeking detail oriented analytical individual ...      1   \n",
       "\n",
       "                                       combined_text  \n",
       "0  summarya business management graduate signific...  \n",
       "1  professional profilecapable international tax ...  \n",
       "2  professional profilehighly motivated sales ass...  \n",
       "3  summaryorganized motivated employee eager appl...  \n",
       "4  summaryemployed navy civilian electrical engin...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- DATA LOADERS -----------------\n",
    "\n",
    "train_dataset = SentencePairDataset(processed_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "eval_dataset = SentencePairDataset(processed_eval, tokenizer)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "test_dataset = SentencePairDataset(processed_test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size * 3, 3)  # 3 classes for classification\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "        outputs_a = self.bert(input_ids_a, attention_mask=attention_mask_a)\n",
    "        pooled_output_a = outputs_a.last_hidden_state[:, 0, :]  # CLS token for sentence A\n",
    "\n",
    "        outputs_b = self.bert(input_ids_b, attention_mask=attention_mask_b)\n",
    "        pooled_output_b = outputs_b.last_hidden_state[:, 0, :]  # CLS token for sentence B\n",
    "\n",
    "        abs_diff = torch.abs(pooled_output_a - pooled_output_b)\n",
    "        combined = torch.cat((pooled_output_a, pooled_output_b, abs_diff), dim=1)\n",
    "        logits = self.fc(combined)\n",
    "\n",
    "        return logits  # Return raw logits; softmax will be applied in loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available devices and assign the appropriate one\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SBERT model and send it to the selected device\n",
    "sbert = SBERT(model).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(sbert.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrandhi1\u001b[0m (\u001b[33mmakeai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/cse-404-fs24-jobfit/models/wandb/run-20241123_205210-a51wfql7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/makeai/jobfit/runs/a51wfql7' target=\"_blank\">sbert-training-1</a></strong> to <a href='https://wandb.ai/makeai/jobfit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/makeai/jobfit' target=\"_blank\">https://wandb.ai/makeai/jobfit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/makeai/jobfit/runs/a51wfql7' target=\"_blank\">https://wandb.ai/makeai/jobfit/runs/a51wfql7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define and initialize wandb\n",
    "config = {\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epochs\": 20,\n",
    "    \"model\": \"distilbert-base-uncased\",\n",
    "    \"dataset\": \"processed_train\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"jobfit\",\n",
    "    config=config,\n",
    "    name=\"sbert-training-1\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "# Access the config\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1129.0325\n",
      "Epoch 1, Eval Loss: 223.4205, Eval Accuracy: 0.6541\n",
      "Epoch 2, Train Loss: 828.9441\n",
      "Epoch 2, Eval Loss: 190.7120, Eval Accuracy: 0.7318\n",
      "Epoch 3, Train Loss: 709.2908\n",
      "Epoch 3, Eval Loss: 169.2036, Eval Accuracy: 0.7518\n",
      "Epoch 4, Train Loss: 605.6866\n",
      "Epoch 4, Eval Loss: 160.8488, Eval Accuracy: 0.7870\n",
      "Epoch 5, Train Loss: 512.0554\n",
      "Epoch 5, Eval Loss: 152.7054, Eval Accuracy: 0.7974\n",
      "Epoch 6, Train Loss: 432.3302\n",
      "Epoch 6, Eval Loss: 139.5454, Eval Accuracy: 0.8247\n",
      "Epoch 7, Train Loss: 368.8202\n",
      "Epoch 7, Eval Loss: 127.9034, Eval Accuracy: 0.8519\n",
      "Epoch 8, Train Loss: 324.3971\n",
      "Epoch 8, Eval Loss: 130.0263, Eval Accuracy: 0.8495\n",
      "Epoch 9, Train Loss: 282.1511\n",
      "Epoch 9, Eval Loss: 124.1237, Eval Accuracy: 0.8575\n",
      "Epoch 10, Train Loss: 246.2789\n",
      "Epoch 10, Eval Loss: 126.7270, Eval Accuracy: 0.8591\n",
      "Epoch 11, Train Loss: 213.1448\n",
      "Epoch 11, Eval Loss: 119.4215, Eval Accuracy: 0.8719\n",
      "Epoch 12, Train Loss: 202.7066\n",
      "Epoch 12, Eval Loss: 132.6694, Eval Accuracy: 0.8711\n",
      "Epoch 13, Train Loss: 187.2588\n",
      "Epoch 13, Eval Loss: 133.5500, Eval Accuracy: 0.8591\n",
      "Epoch 14, Train Loss: 165.5736\n",
      "Epoch 14, Eval Loss: 123.6026, Eval Accuracy: 0.8863\n",
      "Epoch 15, Train Loss: 158.8247\n",
      "Epoch 15, Eval Loss: 133.7383, Eval Accuracy: 0.8751\n",
      "Epoch 16, Train Loss: 145.2925\n",
      "Epoch 16, Eval Loss: 147.1909, Eval Accuracy: 0.8807\n",
      "Epoch 17, Train Loss: 129.3715\n",
      "Epoch 17, Eval Loss: 135.5854, Eval Accuracy: 0.8823\n",
      "Epoch 18, Train Loss: 119.1636\n",
      "Epoch 18, Eval Loss: 135.2897, Eval Accuracy: 0.8855\n",
      "Epoch 19, Train Loss: 118.7032\n",
      "Epoch 19, Eval Loss: 150.8983, Eval Accuracy: 0.8719\n",
      "Epoch 20, Train Loss: 106.2731\n",
      "Epoch 20, Eval Loss: 133.9353, Eval Accuracy: 0.8823\n"
     ]
    }
   ],
   "source": [
    "# Training loop with eval set included\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    sbert.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": loss / len(train_loader)\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on eval set\n",
    "    sbert.eval()\n",
    "    eval_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids_a = batch['input_ids_a'].to(device)\n",
    "            attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "            input_ids_b = batch['input_ids_b'].to(device)\n",
    "            attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    eval_accuracy = correct_predictions / total_predictions\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"eval_loss\": eval_loss,\n",
    "        \"eval_accuracy\": eval_accuracy\n",
    "    })\n",
    "    print(f\"Epoch {epoch + 1}, Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 752.4419, Test Accuracy: 0.5458\n"
     ]
    }
   ],
   "source": [
    "# Final test performance check\n",
    "sbert.eval()\n",
    "test_loss = 0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "test_accuracy = correct_predictions / total_predictions\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "wandb.log({\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>eval_accuracy</td><td>▁▃▄▅▅▆▇▇▇▇██▇███████</td></tr><tr><td>eval_loss</td><td>█▆▄▄▃▂▂▂▁▁▁▂▂▁▂▃▂▂▃▂</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>█▄█▇▅▆▃▄▂▃▁▃▃▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>eval_accuracy</td><td>0.88231</td></tr><tr><td>eval_loss</td><td>133.93529</td></tr><tr><td>test_accuracy</td><td>0.54576</td></tr><tr><td>test_loss</td><td>752.44194</td></tr><tr><td>train_loss</td><td>2e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sbert-training-1</strong> at: <a href='https://wandb.ai/makeai/jobfit/runs/a51wfql7' target=\"_blank\">https://wandb.ai/makeai/jobfit/runs/a51wfql7</a><br/> View project at: <a href='https://wandb.ai/makeai/jobfit' target=\"_blank\">https://wandb.ai/makeai/jobfit</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241123_205210-a51wfql7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below can be used to checkpoint/save the weights of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "# }, \"model_and_optimizer_nn_1.pth\")\n",
    "\n",
    "\n",
    "# Load both model and optimizer state_dicts\n",
    "# checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobfit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
