{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re \n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "le = LabelEncoder()\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     sentence=str(sentence)\n",
    "#     sentence = sentence.lower()\n",
    "#     sentence=sentence.replace('{html}',\"\")\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', sentence)\n",
    "#     rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "#     rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(rem_num)  \n",
    "#     filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "#     stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "#     lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "#     return \" \".join(filtered_words)\n",
    "\n",
    "# print(\"--------PROCESSING TRAINING DATA--------\")\n",
    "# processed_df = df.copy()\n",
    "# processed_df['resume_text'] = processed_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['job_description_text'] = processed_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_df['combined_text'] = processed_df['resume_text'] + ' ' + processed_df['job_description_text']\n",
    "# processed_df['label'] = le.fit_transform(processed_df['label'])\n",
    "\n",
    "# print(\"--------PROCESSING TEST DATA--------\")\n",
    "# test_df = pd.read_csv('data/test.csv')\n",
    "# processed_test_df = test_df.copy()\n",
    "\n",
    "# processed_test_df['resume_text'] = processed_test_df['resume_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['job_description_text'] = processed_test_df['job_description_text'].map(lambda s: preprocess(s))\n",
    "# processed_test_df['combined_text'] = processed_test_df['resume_text'] + ' ' + processed_test_df['job_description_text']\n",
    "# processed_test_df['label'] = le.transform(processed_test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_df.to_csv('data/processed_train.csv', index=False)\n",
    "# processed_test_df.to_csv('data/processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making an evaluation set from the training set\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_processed = pd.read_csv('data/processed_train.csv')\n",
    "# train_data, eval_data = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "# train_data.to_csv('data/processed_train.csv', index=False)\n",
    "# eval_data.to_csv('data/processed_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.read_csv('data/processed_train.csv')\n",
    "processed_test_df = pd.read_csv('data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# # Load a pretrained model (BERT in this case) and its tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        df: Pandas DataFrame with columns 'resume_text', 'job_description_text', and 'label'\n",
    "        \"\"\"\n",
    "        self.sentences_a = df['resume_text'].tolist()\n",
    "        self.sentences_b = df['job_description_text'].tolist()\n",
    "        self.labels = df['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the resume (sentence_a) and job description (sentence_b)\n",
    "        encoding_a = self.tokenizer(\n",
    "            self.sentences_a[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding_b = self.tokenizer(\n",
    "            self.sentences_b[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create a dictionary with input_ids and attention_mask for both inputs\n",
    "        item = {\n",
    "            'input_ids_a': encoding_a['input_ids'].squeeze(0),  # Resume input\n",
    "            'attention_mask_a': encoding_a['attention_mask'].squeeze(0),  # Resume attention mask\n",
    "            'input_ids_b': encoding_b['input_ids'].squeeze(0),  # Job description input\n",
    "            'attention_mask_b': encoding_b['attention_mask'].squeeze(0),  # Job description attention mask\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)  # Labels (classification)\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume_text</th>\n",
       "      <th>job_description_text</th>\n",
       "      <th>label</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summaryhighly motivated sales associate extens...</td>\n",
       "      <td>netsource inc award winning total workforce so...</td>\n",
       "      <td>1</td>\n",
       "      <td>summaryhighly motivated sales associate extens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>professional summarycurrently working caterpil...</td>\n",
       "      <td>salas obrien tell clients engineered impact pa...</td>\n",
       "      <td>1</td>\n",
       "      <td>professional summarycurrently working caterpil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>summaryi started construction career june jack...</td>\n",
       "      <td>schweitzer engineering laboratories sel infras...</td>\n",
       "      <td>1</td>\n",
       "      <td>summaryi started construction career june jack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summarycertified electrical foremanwith thirte...</td>\n",
       "      <td>mizick miller company inc looking dynamic indi...</td>\n",
       "      <td>1</td>\n",
       "      <td>summarycertified electrical foremanwith thirte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summarywith extensive experience business requ...</td>\n",
       "      <td>life capgemini capgemini supports aspects well...</td>\n",
       "      <td>1</td>\n",
       "      <td>summarywith extensive experience business requ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         resume_text  \\\n",
       "0  summaryhighly motivated sales associate extens...   \n",
       "1  professional summarycurrently working caterpil...   \n",
       "2  summaryi started construction career june jack...   \n",
       "3  summarycertified electrical foremanwith thirte...   \n",
       "4  summarywith extensive experience business requ...   \n",
       "\n",
       "                                job_description_text  label  \\\n",
       "0  netsource inc award winning total workforce so...      1   \n",
       "1  salas obrien tell clients engineered impact pa...      1   \n",
       "2  schweitzer engineering laboratories sel infras...      1   \n",
       "3  mizick miller company inc looking dynamic indi...      1   \n",
       "4  life capgemini capgemini supports aspects well...      1   \n",
       "\n",
       "                                       combined_text  \n",
       "0  summaryhighly motivated sales associate extens...  \n",
       "1  professional summarycurrently working caterpil...  \n",
       "2  summaryi started construction career june jack...  \n",
       "3  summarycertified electrical foremanwith thirte...  \n",
       "4  summarywith extensive experience business requ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "dataset = SentencePairDataset(processed_df, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)  # Adjust batch size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(SBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size * 3, 3)  # 3 classes for classification\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax for classification\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "        # Get the BERT outputs for both sentence pairs\n",
    "        outputs_a = self.bert(input_ids_a, attention_mask=attention_mask_a)\n",
    "        pooled_output_a = outputs_a.last_hidden_state[:, 0, :]  # CLS token for sentence A\n",
    "\n",
    "        outputs_b = self.bert(input_ids_b, attention_mask=attention_mask_b)\n",
    "        pooled_output_b = outputs_b.last_hidden_state[:, 0, :]  # CLS token for sentence B\n",
    "\n",
    "        # Compute the element-wise absolute difference |u - v|\n",
    "        abs_diff = torch.abs(pooled_output_a - pooled_output_b)\n",
    "\n",
    "        # Concatenate u, v, and |u - v|\n",
    "        combined = torch.cat((pooled_output_a, pooled_output_b, abs_diff), dim=1)\n",
    "\n",
    "        # Pass through a fully connected layer\n",
    "        logits = self.fc(combined)\n",
    "\n",
    "        # Apply softmax to get classification probabilities\n",
    "        probabilities = self.softmax(logits)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "        # Apply softmax to get classification probabilities\n",
    "        probabilities = self.softmax(logits)\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1533.9761\n",
      "Epoch 2, Loss: 1356.0796\n",
      "Epoch 3, Loss: 1281.2727\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check for available devices and assign the appropriate one\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize SBERT model and send it to the selected device\n",
    "sbert = SBERT(model).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification\n",
    "optimizer = optim.Adam(sbert.parameters(), lr=2e-5)\n",
    "\n",
    "# Set the accumulation steps (e.g., accumulate gradients over 4 smaller batches)\n",
    "accumulation_steps = 4\n",
    "\n",
    "# Training loop\n",
    "sbert.train()\n",
    "for epoch in range(3):  # Train for 3 epochs (adjust as needed)\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move input data to the same device as the model (MPS, CUDA, or CPU)\n",
    "        input_ids_a = batch['input_ids_a'].to(device)  # Resume input\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)  # Attention mask for resume\n",
    "        input_ids_b = batch['input_ids_b'].to(device)  # Job description input\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)  # Attention mask for job description\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = sbert(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobfit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
